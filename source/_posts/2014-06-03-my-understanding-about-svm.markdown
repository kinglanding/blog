---
layout: post
title: "从逻辑回归分类到SVM分类"
date: 2014-06-03 20:53
comments: true
tags: 
- 机器学习
- SVM
- Machine Learning
---

上一章讲到了线性回归的一个典型算法，核心思想就是利用最小二乘法最为损失函数，不断使用梯度下降法（或者使用随机梯度下降法（stochastic gradient descent））来更新theta值。

后来降到了分类，逻辑回归只不过有个很好的性质就是值分布在0到1之间，正好可以利用到分类上。logistic回归就是要学习得到θ，使得正例的特征远大于 0，负例的特征远
小于 0，强调在全部训练实例上达到这个目标。为什么说逻辑回归是个线性模型？这是因为该模型是将特性的线性组合作为自变量，然后使用logistic函数（或者说是sigmoid函数）将自变量映射到（0-1）上，将值和概率结合之后从而应用到分类上。
函数表示形式：

$$
h_{ \theta  }\left( x \right) =g\left( \theta ^{ T }x \right) =\frac { 1 }{ 1+{e  }^{ -\theta^{T}x } }
$$

<!-- more -->

其中 x 是 n 维特征向量，函数 g 就是 logistic 函数
从线性回归到了逻辑回归，从逻辑回归又到了分类，那么再来看看SVM这个有监督的分类学习算法。
在这里我们使用的y的取值记为1和-1，所以对logistic 回归中的做下替换，令logistic回归中的y=0和y=1变为y=-1,y=1。同时将θ替换成 w 和 b。
所以有$$\Theta^{T}x=\theta_0x_0+\theta_1x_1+\cdots +\theta_nx_n$$，现在使用b替换$$\theta_0$$,替换后的形式变为$$w^{ T }x=w_{ 1 }x_{ 1 }+\cdots +w_{ n }x_{ n }+b$$，这样做之后，我们的假设函数就变成了

$$
h_{ b,w }\left( x \right) =g\left( w^{ T }x+b \right)
$$

和逻辑回归的形式很相像。

表达形式就引申到这里，在来谈下SVM的思想，当我们学习线性回归时，我们的想法就是使用最小二乘法拟合数据，而在分类问题中，我们的想法就是找到一条直线，使正负样本离这个线或者超平面尽可能的远。也就是`间隔`最大。用一句话来说就是：**在特征空间上的间隔最大的线性分类器。所以我们所有的努力都在如何是间隔最大化上，而这个可以转化为一个凸二次规划的问题**。所以SVM的学习算法就是求解凸二次规划的最优化算法。

#### 函数间隔（functional margin）和几何间隔（geometric margin）

我们定义函数间隔就是：对于给定的数据集T和超平面（w,b），样本点$$\left( x^{(i)},y^{(i)} \right) $$到超平面的函数间隔为：
$$
\widehat { \gamma  } ^{ (i) }=y^{ (i) }(w^{T}\cdot x^{ (i) }+b) 
$$

函数间隔或者间隔本身描述了一种确信度。离超平面越远，间隔值就越大，可信度就越大。

刚刚我们定义的函数间隔是针对某一个样本的，现在我们定义全局样本上的函数间隔，定义超平面关于数据集的函数间隔为超平面中所有样本点函数间隔的最小值，就是在训练样本上分类正例和负例确信度最小那个函数间隔，即

$$
\widehat {\gamma }=\min _{ i=1,...m }{ \widehat { \gamma  } ^{ (i) } }
$$

但是有个问题，如果按比例的增加w和b，那么函数间隔也会按比例改变，这个对结果没有影响，但是问题会变得不好描述，不能定量的计算，所以我们就需要把它规范化(normalization)。只需要结果除以$$\left\| w \right\| $$就好了，这个时候$$w/\left\| w \right\| $$就成为了单位向量，所以函数间隔和几何间隔的关系也就是这样简单，几何间隔就是规范化后的函数间隔。无论w和b怎么折腾，几何间隔都不会改变。

定义几何间隔就是：对于给定的数据集T和超平面（w,b），样本点$$\left( x^{(i)},y^{(i)} \right) $$到超平面的几何间隔为：

$$
{ \gamma  } ^{ (i) }=y^{ (i) }(w^{T}\cdot x^{ (i) }+b)/\left\| w \right\|
$$

定义超平面关于数据集的几何间隔为超平面中所有样本点几何间隔的最小值,即

$$
{ \gamma  }=\min _{ i=1,...m }{ { \gamma  }^{ (i) } }
$$

最优间隔分类器（optimal margin classifier）（利用间隔最大化）

回想前面我们提到我们的目标是寻找一个超平面，使得离超平面比较近的点能有更大的
间距。 也就是我们不考虑所有的点都必须远离超平面，我们关心求得的超平面能够让所有点中离它最近的点具有最大间距。形象的说，我们将上面的图看作是一张纸，我们要找一条折线，按照这条折线折叠后，离折线最近的点的间距比其他折线都要大。形式化表示为：

$$
\max _{ \gamma ,w,b }{ \gamma  } \\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) \ge \gamma ,i=1,\cdots ,m\\ \left\| w \right\| =1
$$


这里用||w||=1 规约 w，使得$$ w^{T}\cdot x+b$$是几何间隔。

到此，我们已经将模型定义出来了。如果求得了 w 和 b，那么来一个特征 x，我们就能
够分类了，称为最优间隔分类器。接下的问题就是如何求解 w 和 b 的问题了。

由于||w|| = 1不是凸函数，我们想先处理转化一下，考虑几何间隔和函数间隔的关系，
$$ \gamma =\frac { \hat { \gamma  }  }{ \left\| w \right\|  } $$，我们改写一下上面的式子：

$$
\max _{ \gamma ,w,b }{ \frac { \widehat { \gamma  }  }{ \left\| w \right\|  }  } \\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) \ge \widehat { \gamma  } ,i=1,\cdots ,m
$$
 
因为函数间隔值得改变对结果没有影响，所以可以给它个固定值比如1.将 $$\hat { \gamma  } =1$$代入上面的最优化问题，因为最大化$$ \frac { 1 }{ \left\| w \right\|  } $$最小化$$ \frac { 1 }{ 2 } \left\| w \right\| ^{ 2 }$$是等价的，于是将上面改写成这样：

$$
\min _{ \gamma ,w,b }{ \frac { 1 }{ 2 }  } { \left\| w \right\|  }^{ 2 }\\ s.t\quad { y }^{ \left( i \right)  }\left( { w }^{ T }{ x }^{ \left( i \right)  }+b \right) -1\ge 0,i=1,\cdots ,m
$$
 
这就变成了一个凸二次规划问题，详情见[凸二次规划](http://zh.wikipedia.org/wiki/%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92)


接下来是关于拉格朗日对偶的问题，在这之后，在描述SVM中最简单的分类器——**线性可分支持向量机**，因为这个情况下，数据是线性可分的，而且噪音没有，只需要通过**硬间隔最大化**,即可学习一个线性的分类器，又称硬间隔支持向量机。

> 参考

1. Andrew Ng的原始课件讲义

2. 统计学习方法
